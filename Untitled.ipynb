{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matibudd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\matibudd\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-56ee50da2999>:76: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "0\n",
      "1\n",
      "0.0036796536\n",
      "\n",
      "\n",
      "2\n",
      "0.0015151515\n",
      "\n",
      "\n",
      "3\n",
      "0.00036075036\n",
      "\n",
      "\n",
      "4\n",
      "0.00021645022\n",
      "\n",
      "\n",
      "5\n",
      "0.0\n",
      "\n",
      "\n",
      "6\n",
      "0.0\n",
      "\n",
      "\n",
      "7\n",
      "0.0\n",
      "\n",
      "\n",
      "8\n",
      "0.0\n",
      "\n",
      "\n",
      "9\n",
      "0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "class data_loader():\n",
    "    train_file = \"\"\n",
    "    test_file = \"\"\n",
    "    def __init__(self,train_file,test_file):\n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "    def load_data(self):\n",
    "        train_df = pd.read_csv(self.train_file)\n",
    "        test_df = pd.read_csv(self.test_file)\n",
    "        train_label = train_df['label']\n",
    "        train_df.drop(['label'],inplace=True,axis=1)\n",
    "        train_label_vec = np.zeros([len(train_label),10],int)\n",
    "        for index in range(len(train_label)):\n",
    "            train_label_vec[index] = train_label[index]\n",
    "        return train_df,train_label_vec,test_df\n",
    "    def visualize_mnist_data(self,tr_df,ts_df,index):\n",
    "        plt.imshow(tr_df.iloc[index].reshape(28,28))\n",
    "        return\n",
    "class network_operations():\n",
    "    #def __init__(self)\n",
    "    def init_weights(self,shape):\n",
    "        W = tf.Variable(tf.truncated_normal(shape=shape))\n",
    "        return W\n",
    "    def init_bias(self,shape):\n",
    "        b = tf.Variable(tf.constant(0.1,shape=shape))\n",
    "        return b\n",
    "    def conv(self,X,f_shape):\n",
    "        F = self.init_weights(f_shape)\n",
    "        b = self.init_bias([f_shape[3]])\n",
    "        return tf.nn.relu(tf.nn.conv2d(X,F,strides=[1,1,1,1],padding='SAME') + b)\n",
    "    def dense_layer(self,X,num_neurons):\n",
    "        input_size = int(X.get_shape()[1])\n",
    "        W = self.init_weights([input_size,num_neurons])\n",
    "        b = self.init_bias([num_neurons])\n",
    "        return tf.nn.relu(tf.matmul(X,W) + b)\n",
    "    def poll(self,X):\n",
    "        return tf.nn.max_pool(X,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    def dropout(self,X,p):\n",
    "        return tf.nn.dropout(X,keep_prob=p)\n",
    "def get_next_batch(X,y,batch_size,batch_num):\n",
    "    start_ix = batch_num*batch_size;\n",
    "    stop_ix = min((batch_num+1)*batch_size,len(X))\n",
    "    return X.iloc[start_ix:stop_ix],y[start_ix:stop_ix]\n",
    "def main():\n",
    "    #load_data\n",
    "    d_l = data_loader(\"train.csv\",\"test.csv\")\n",
    "    train_df,train_label,test_df = d_l.load_data()\n",
    "    d_l.visualize_mnist_data(train_df,test_df,10)\n",
    "    #create graph\n",
    "    X = tf.placeholder(dtype=float,shape=[None,784])\n",
    "    y = tf.placeholder(dtype=float,shape=[None,10])\n",
    "    x_image = tf.reshape(X,[-1,28,28,1])\n",
    "    net_op = network_operations()\n",
    "    #Layer_1\n",
    "    x_image_l1_bar = net_op.conv(x_image,[5,5,1,32])\n",
    "    x_image_l1     = net_op.poll(x_image_l1_bar)\n",
    "    #Layer_1\n",
    "    x_image_l2_bar = net_op.conv(x_image_l1,[5,5,32,64])\n",
    "    x_image_l2     = net_op.poll(x_image_l2_bar)\n",
    "    #Flatten image\n",
    "    flat_image = tf.reshape(x_image_l2,[-1,7*7*64])\n",
    "    #Connected layer 1\n",
    "    full_layer_op = net_op.dense_layer(flat_image,1024)\n",
    "    #Dropout\n",
    "    p = tf.placeholder(tf.float32)\n",
    "    full_layer_op_drop = net_op.dropout(full_layer_op,p)\n",
    "    #y_pred\n",
    "    y_pred = net_op.dense_layer(full_layer_op_drop,10)\n",
    "    #loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_pred))\n",
    "    #optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train = optimizer.minimize(loss)\n",
    "    #init global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    epochs = 10\n",
    "    batch_size = 50\n",
    "    batch_num = 0\n",
    "    #start session\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_df, train_label, test_size=0.33, random_state=42)    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(epochs):\n",
    "            print(i)\n",
    "            batch_x,batch_y  = get_next_batch(X_train,y_train,batch_size,batch_num)\n",
    "            batch_num = batch_num + 1\n",
    "            sess.run(train,feed_dict = {X:batch_x,y:batch_y,p:0.5})\n",
    "            if(i%100):\n",
    "                matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y,1))\n",
    "                acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "                print(sess.run(acc,feed_dict={X:X_test,y:y_test,p:1.0}))\n",
    "                print(\"\\n\");\n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
